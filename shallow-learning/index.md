---
layout: post
title: Encoder-decoder
---
In graph-based deep learning, low-dimensional vector representations of nodes in a graph are known as node embeddings. The goal of node embeddings is to project graph nodes from their original feature space into lower-dimensional space to reduce computational complexity while retaining the structural and relational information of nodes in the graph. A key idea in graph-based deep learning, or more precisely in graph representation learning, is to learn these node embeddings.
## 1. Encoder-decoder framework
In graph representation learning, we leverage the encoder-decoder framework to facilitate a comprehensive understanding of the graph structure. It is achieved in two crucial steps. Firstly, an encoder model is employed to map each node in the graph to a compact, low-dimensional vector, known as embedding. These embeddings are then passed as input to a decoder model that aims to reconstruct the local neighbourhood information for each node in the original graph. By doing so, we obtain a rich and structured graph representation conducive to further analysis. The encoder-decoder framework is depicted in Figure 1.
 
 ### Encoder: 
 The encoder can be interpreted as a mapping function that transforms node $v \in V$ in the graph into a vector representation $Z_v \in \mathbb{R} ^d$ where $Z_v$ represents the embedding for node $v \in V$. The encoder takes node IDs as input and generates node embeddings corresponding to the node. Hence,$ ENC: V \to \mathbb{R} ^d $
        The encoder typically utilizes a shallow embedding approach, performing an embedding lookup based on the node ID. \textit{Shallow embedding approaches}\footnote{We will only be concentrating on shallow-embedding techniques throughout this module; however, deep-learning-based embedding approaches, such as GCN, can be explored independently.} are where an encoder that maps each node to a unique embedding is just a simple lookup function (equation \ref{eqn 2}) without considering any node features and can be stated as:
        \begin{equation}\label{eqn 2}
        ENC(v) = Z[v]
        \end{equation}
        where matrix \begin{equation} Z \in \mathbb{R} ^{\left| V \right| \times d} \end{equation} is a matrix that contains $d$-dimensional embedding vectors for all nodes in graph and $Z[v]$ is a row in matrix $Z$ corresponding to node $v$. 
        \item \textbf{Decoder:}
        The decoder takes as input the node embeddings generated by the encoder and tries to recover graph statistics from these node embeddings. As an illustration, suppose the node embeddings corresponding to node $v$ is $z_v$; the decoder may predict a node $v's$ neighbour set $N(u)$ or row corresponding to node 
        $v$ in adjacency matrix of graph $A[v]$.
        A simple approach for implementing a decoder is to use pairwise decoders, which try to predict the relationship or similarity between the pair of nodes. Formally it can be stated as: 
        \begin{equation}
            DEC: \mathbb{R}^d \times \mathbb{R}^d \to \mathbb{R}^+ 
        \end{equation}
        Consider an example case where a pairwise decoder is employed to make predictions regarding the adjacency of nodes in the graph. This decoder examines pairs of nodes and predicts whether they are connected by an edge in the graph. Therefore, the relationship between the nodes $u$ and $v$ is reconstructed when the pairwise decoder is applied to the pair of their corresponding node embeddings ($z_u$, $z_v$). The ultimate goal of decoder is to minimize the difference between the it's output ($DEC(z_u, z_v)$) and the graph-based similarity measure ($S[u,v]$). That is:
        \begin{equation}\label{eqn 4}
            DEC(ENC(u), ENC(v)) = DEC(z_u, z_v) \sim S[u,v]
        \end{equation}
        A common example of a node similarity-based matrix, $S[u,v]$, is the adjacency matrix, $A[u,v]$ of the graph.

        \end{itemize}    


